<!doctype html><html class="theme-next pisces use-motion"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="fonts.useso.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css"><meta name="keywords" content="机器学习,sklearn,中文文本分类,"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1"><meta name="description" content="一、基本概念文本分类是文本挖掘中的一个重要领域，所谓文本分类，事实上指的是为用户给出每个文档，文本片段等所要归属于哪一类(类别，主题等)的问题。文本挖掘  文本挖掘(Text Mining)是从非结构化文本信息中获取用户感兴趣或者有用的模式 的过程。其中被普遍认可的文本挖掘定义如下:文本挖掘是指从大量文本数据中抽取事先未知的、可理解的、最终可用的知识的过程，同时运用这些知识更好地组织信息以便将来参"><meta name="keywords" content="机器学习,sklearn,中文文本分类"><meta property="og:type" content="article"><meta property="og:title" content="ML：sklearn中文文本分类"><meta property="og:url" content="http://yoursite.com/2017/11/15/ML：sklearn中文文本分类.html"><meta property="og:site_name" content="ilioner"><meta property="og:description" content="一、基本概念文本分类是文本挖掘中的一个重要领域，所谓文本分类，事实上指的是为用户给出每个文档，文本片段等所要归属于哪一类(类别，主题等)的问题。文本挖掘  文本挖掘(Text Mining)是从非结构化文本信息中获取用户感兴趣或者有用的模式 的过程。其中被普遍认可的文本挖掘定义如下:文本挖掘是指从大量文本数据中抽取事先未知的、可理解的、最终可用的知识的过程，同时运用这些知识更好地组织信息以便将来参"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2017-11-16T03:31:38.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="ML：sklearn中文文本分类"><meta name="twitter:description" content="一、基本概念文本分类是文本挖掘中的一个重要领域，所谓文本分类，事实上指的是为用户给出每个文档，文本片段等所要归属于哪一类(类别，主题等)的问题。文本挖掘  文本挖掘(Text Mining)是从非结构化文本信息中获取用户感兴趣或者有用的模式 的过程。其中被普遍认可的文本挖掘定义如下:文本挖掘是指从大量文本数据中抽取事先未知的、可理解的、最终可用的知识的过程，同时运用这些知识更好地组织信息以便将来参"><script type="text/javascript" id="hexo.configuration">var NexT=window.NexT||{},CONFIG={scheme:"Pisces",sidebar:{position:"left",display:"hide"},fancybox:!0,motion:!0,duoshuo:{userId:0,author:"博主"}}</script><title>ML：sklearn中文文本分类 | ilioner</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">ilioner</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">为什么坚持，想想当初</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-search"><a href="#" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><header class="post-header"><h1 class="post-title" itemprop="name headline">ML：sklearn中文文本分类</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time itemprop="dateCreated" datetime="2017-11-15T13:28:00+08:00" content="2017-11-15">2017-11-15 </time></span><span class="post-category">&nbsp; | &nbsp; <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span> </a></span></span><span class="post-comments-count">&nbsp; | &nbsp; <a href="/2017/11/15/ML：sklearn中文文本分类.html#comments" itemprop="discussionUrl"><span class="post-comments-count ds-thread-count" data-thread-key="2017/11/15/ML：sklearn中文文本分类.html" itemprop="commentsCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h3><p>文本分类是文本挖掘中的一个重要领域，所谓文本分类，事实上指的是为用户给出每个文档，文本片段等所要归属于哪一类(类别，主题等)的问题。</p><pre><code>文本挖掘

文本挖掘(Text Mining)是从非结构化文本信息中获取用户感兴趣或者有用的模式 的过程。其中被普遍认可的文本挖掘定义如下:文本挖掘是指从大量文本数据中抽取事先未知的、可理解的、最终可用的知识的过程，同时运用这些知识更好地组织信息以便将来参考。
　　简言之，文本挖掘就是从非结构化的文本中寻找知识的过程。 文本挖掘的七个主要领域：
　　（1）搜索和信息检索（IR）：存储和文本文档的检索，包括搜索引擎和关键字搜 索。
　　（2）文本聚类：使用聚类方法，对词汇，片段，段落或文件进行分组和归类。
　　（3）文本分类：对片段，段落或文件进行分组和归类，使用数据挖掘分类方法的 基础上，经过训练的标记示例模型。
　　（4）Web 挖掘：在互联网上进行数据和文本挖掘，并特别关注在网络的规模和相 互联系。
　　（5）信息抽取（IE）：从非结构化文本中识别与提取有关的事实和关系;从非结构 化和半结构化文本制作的结构化数据的过程。
　　（6）自然语言处理（NLP）：将语言作为一种有意义、有规则的符号系统，在底 层解析和理解语言的任务（例如，词性标注）;目前的技术主要从语法、语义 的角度发现语言最本质的结构和所表达的意义。
　　（7）概念提取：把单词和短语按语义分组成意义相似的组。
</code></pre><p>文本分类的应用比较广泛，如:垃圾邮件检测，文件归档，网页分层等等。</p><h3 id="二、基本步骤"><a href="#二、基本步骤" class="headerlink" title="二、基本步骤"></a>二、基本步骤</h3><p>1.预处理<br>2.词向量处理<br>3.TF-IDF权重策略<br>4.建立分类器<br>5.评价分类结果</p><p><strong>中文特殊情况</strong></p><p>1.预处理<br>2.分词处理<br>3.词向量处理<br>4.TF-IDF权重策略<br>5.建立分类器<br>6.评价分类结果</p><h5 id="1-预处理"><a href="#1-预处理" class="headerlink" title="1.预处理"></a>1.预处理</h5><p>主要实现对文本内容的降噪，划分训练集、测试集等过程。<br>比如，抓取了200篇类别划分明确的百度百科，首先要去除内容中的一些html标签，留下需要用到的内容，删除不必要的内容等等。然后将这些内容进行训练集于测试集的划分。</p><h5 id="2-分词处理"><a href="#2-分词处理" class="headerlink" title="2.分词处理"></a>2.分词处理</h5><p>相对于英文而言，中文分词有一定的难点，将连续的汉字序列切分成一个个的单词要考虑很多因素，比如句法，精度等等。</p><pre><code>比如:火车上有人
正确分词结果:火车 上 有 人
有歧义的结果:火 车上 有 人
</code></pre><p>分词的精度将直接导致文本的关键信息以至于对最后的分类结果造成影响。</p><p>目前已经有比较成熟的分词工具被提供使用:<br>具体可以参照<a href="http://blog.csdn.net/sinat_26917383/article/details/77067515" target="_blank" rel="noopener">一些比较好用的中文分词器</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 获取分类</span><br><span class="line">data_list = os.listdir(&apos;./data&apos;) #语料数据</span><br><span class="line">seg_path = &apos;./segment&apos; #分词后的数据</span><br><span class="line">seg_train_path = &apos;./segment/train&apos; #分词后的训练数据</span><br><span class="line">seg_test_path = &apos;./segment/test&apos;#分词后的测试数据</span><br><span class="line"></span><br><span class="line"># 获取各个分类内容，并进行切词保存在segment目录下</span><br><span class="line">def segment_content():</span><br><span class="line">    for data_cate in data_list:</span><br><span class="line">        content = &quot;&quot;</span><br><span class="line">        if not os.path.isdir(data_cate) and not data_cate == &apos;.DS_Store&apos;:</span><br><span class="line">            path = &apos;./data/%s&apos;%(data_cate)</span><br><span class="line">            files= os.listdir(path)</span><br><span class="line">            for file in files:</span><br><span class="line">                file_path = &apos;./data/%s/%s&apos;%(data_cate,file)</span><br><span class="line">                print(file_path)</span><br><span class="line">                if file == &quot;.DS_Store&quot;:</span><br><span class="line">                    os.remove(file_path)</span><br><span class="line">                else:</span><br><span class="line">                    f = open(file_path, mode=&apos;r&apos;, encoding=&apos;GB18030&apos;)</span><br><span class="line">                    try:</span><br><span class="line">                        content = f.read()</span><br><span class="line">                    except Exception:</span><br><span class="line">                        print(&apos;无法读取&apos;)</span><br><span class="line">                        continue</span><br><span class="line">                if not os.path.exists(&apos;%s/%s&apos;%(seg_train_path,data_cate)):</span><br><span class="line">                    os.mkdir(&apos;%s/%s&apos;%(seg_train_path,data_cate))</span><br><span class="line">                output = open(&apos;%s/%s/%s&apos;%(seg_train_path,data_cate,file), &apos;w&apos;)</span><br><span class="line">                content_seg = jieba.cut(content) # 使用jieba进行切词</span><br><span class="line">                output.write(&quot; &quot;.join(content_seg))</span><br><span class="line">                output.close()</span><br></pre></td></tr></table></figure><p></p><p>以上的代码，已经将文本进行分词并保存(PS.因为使用了交叉验证，所以没有划分测试集，测试集划分方法可以从已经分好词的文件中随机分出20%也可以先分出20%再进行分词)<br>如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#从切分的数据中分离出测试数据集</span><br><span class="line">def cut_for_test():</span><br><span class="line">    seg_data_list = os.listdir(seg_train_path)</span><br><span class="line">    for data_dir in seg_data_list:</span><br><span class="line">         </span><br><span class="line">        if os.path.isdir(&quot;%s/%s&quot;%(seg_train_path,data_dir)):</span><br><span class="line">            file_dir_path = &quot;%s/%s&quot;%(seg_train_path,data_dir)</span><br><span class="line">            files = os.listdir(file_dir_path)</span><br><span class="line">            test_array = files[-10:]</span><br><span class="line">            for subfile in test_array:</span><br><span class="line">                if subfile == &quot;.DS_Store&quot;:</span><br><span class="line">                    os.remove(&quot;%s/%s&quot;%(file_dir_path,subfile))</span><br><span class="line">                    continue</span><br><span class="line">                if not os.path.exists(&quot;%s/%s&quot;%(seg_test_path,data_dir)):</span><br><span class="line">                    os.mkdir(&quot;%s/%s&quot;%(seg_test_path,data_dir))</span><br><span class="line">                shutil.move(&quot;%s/%s&quot;%(file_dir_path,subfile),&quot;%s/%s/%s&quot;%(seg_test_path,data_dir,subfile))</span><br></pre></td></tr></table></figure><h5 id="3-词向量处理"><a href="#3-词向量处理" class="headerlink" title="3.词向量处理"></a>3.词向量处理</h5><p>在这一步，会将已经分好词的训练集文本进行词频统计，并生成文本词向量空间，</p><p>词向量，顾名思义，就是使用向量来表达词。最常见的表达方式就是”one-hot”，其向量维度为整个语料库中词的总数，每一维代表语料库中的一个词（出现为1，不出现为0）<br>关于这一点可以<a href="http://ilioner.github.io/2017/10/23/sklearn%E4%B8%ADCountVectorizer%E4%B8%8ETfidfTransformer%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B.html" target="_blank" rel="noopener">参考</a></p><p>期间为了更好的进行下一步的TF-IDF的处理，可以将一些类似于（呢，啊，嗯）之类的停用词进行过滤。</p><h5 id="4-TF-IDF权重策略"><a href="#4-TF-IDF权重策略" class="headerlink" title="4.TF-IDF权重策略"></a>4.TF-IDF权重策略</h5><p>使用 TF-IDF 发现特征词，并抽取为反映文档主题 的特征</p><p>可以参照<a href="http://ilioner.github.io/2017/10/23/sklearn%E4%B8%ADCountVectorizer%E4%B8%8ETfidfTransformer%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B.html" target="_blank" rel="noopener">参考</a>，主要工作就是提取文本的关键词信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#第三步，处理词向量，包括去除停用词</span><br><span class="line">def content_vectorizer(data):</span><br><span class="line"></span><br><span class="line">    #载入停用词</span><br><span class="line">    # 语料向量化        </span><br><span class="line">    #if not os.path.exists(&quot;./vectorizer_content.m&quot;):</span><br><span class="line">    vectorizer_content__ = CountVectorizer(stop_words=stopword_list)</span><br><span class="line">    #vectorizer_content = joblib.load(&quot;./vectorizer_content.m&quot;)</span><br><span class="line">    x_array = vectorizer_content__.fit_transform(data)</span><br><span class="line">    #print(x_array.toarray())</span><br><span class="line">    # 计算各个分类内容的 tf-idf值</span><br><span class="line">    X_test =  TfidfTransformer().fit_transform(x_array) </span><br><span class="line">    print(&apos;content_vectorizer&apos;)</span><br><span class="line">    print(vectorizer_content__)</span><br><span class="line">    joblib.dump(vectorizer_content__, &quot;./vectorizer_content.m&quot;) </span><br><span class="line">    return X_test</span><br></pre></td></tr></table></figure><p>如果需要使用测试集进行验证，则需要注意以下信息</p><ul><li><em>测试集在向量化时，需要使用和训练器相同的矢量器 否则会报错 ValueError dimension mismatch</em></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def content_vectorizer_test(data):</span><br><span class="line"></span><br><span class="line">    #载入停用词</span><br><span class="line">    # 语料向量化        </span><br><span class="line">    #if not os.path.exists(&quot;./vectorizer_content.m&quot;):</span><br><span class="line">    #需要使用和训练器相同的矢量器 否则会报错 ValueError dimension mismatch</span><br><span class="line">    vectorizer_content = joblib.load(&quot;./vectorizer_content.m&quot;)</span><br><span class="line">    #vectorizer_content = joblib.load(&quot;./vectorizer_content.m&quot;)</span><br><span class="line">    x_array = vectorizer_content.transform(data)</span><br><span class="line">    #print(x_array.toarray())</span><br><span class="line">    # 计算各个分类内容的 tf-idf值</span><br><span class="line">    X =  TfidfTransformer().fit_transform(x_array) </span><br><span class="line">    print(&apos;content_vectorizer_test&apos;)</span><br><span class="line">    print(vectorizer_content)</span><br><span class="line">    #joblib.dump(vectorizer_content__, &quot;./vectorizer_content.m&quot;) </span><br><span class="line">    return X</span><br></pre></td></tr></table></figure><h5 id="5-建立分类器"><a href="#5-建立分类器" class="headerlink" title="5.建立分类器"></a>5.建立分类器</h5><p>通过上面这些步骤，训练集基本已经处理完成，现在只需要使用合适的分类器进行训练处一个合适的分类器模型(sklearn有很多已经封装好的分类器，只需要按照需求进行调用就好)。</p><h5 id="6-评价分类结果"><a href="#6-评价分类结果" class="headerlink" title="6.评价分类结果"></a>6.评价分类结果</h5><p>训练出来的模型必须进行测试验证，最简单的方式是使用sklearn自带的交叉验证进行评测，当然，也可以按照第一步划分出来的测试集进行测试和验证。当然分值越高说明训练出的模型在处理分类时更加精准。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># 创建分类器</span><br><span class="line"></span><br><span class="line">def train():</span><br><span class="line">    from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">    </span><br><span class="line">    processed_textset,Y = loadtrainset()</span><br><span class="line">    #test_textset,Y_test = loadtestset()</span><br><span class="line">    X_train = content_vectorizer(processed_textset)</span><br><span class="line">    #X_test = content_vectorizer_test(test_textset)</span><br><span class="line">    # 多项式贝叶斯分类器</span><br><span class="line">    clf = MultinomialNB(alpha=0.001).fit(X_train,Y)</span><br><span class="line">    </span><br><span class="line">    # KNN分类器</span><br><span class="line">    #from sklearn.neighbors import KNeighborsClassifier    </span><br><span class="line">    #clf = KNeighborsClassifier().fit(X_train,Y)</span><br><span class="line">    # 随机森林</span><br><span class="line">    #from sklearn.ensemble import RandomForestClassifier    </span><br><span class="line">    #clf = RandomForestClassifier(n_estimators=8)    </span><br><span class="line">    #clf.fit(X_train,Y)  </span><br><span class="line">    </span><br><span class="line">    #predict_result = clf.predict(X_test) #预测结果</span><br><span class="line">    print(&apos;交叉验证结果&apos;)</span><br><span class="line">    print(cross_val_score(clf,X_train,Y,cv=10,scoring=&apos;accuracy&apos;))#交叉验证 #</span><br><span class="line">    </span><br><span class="line">    #训练完成，保存分类器</span><br><span class="line">    joblib.dump(clf, &quot;./分类器.model&quot;) </span><br><span class="line"></span><br><span class="line">#预测</span><br><span class="line">def predict(path):</span><br><span class="line">    check_file_data = file_tools(path)</span><br><span class="line">    check_file_vectorizer = content_vectorizer_test(check_file_data)</span><br><span class="line">    clf = joblib.load(&quot;./分类器.model&quot;)</span><br><span class="line">    predict_result = clf.predict(check_file_vectorizer)</span><br><span class="line">    print(&apos;------&gt;预测结果&lt;------&apos;)</span><br><span class="line">    print(predict_result)</span><br><span class="line">    </span><br><span class="line">#segment_content()</span><br><span class="line">#cut_for_test()</span><br><span class="line">#train()#如果没有训练过模型，需要先通过此方法进行训练    </span><br><span class="line">predict(&apos;./test2.txt&apos;) #test2为随机在网上复制下来的体育类新闻，预测结果为新闻</span><br></pre></td></tr></table></figure><p>记录下完整代码：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python3</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created on Mon Nov  6 15:05:43 2017</span><br><span class="line"></span><br><span class="line">@author: tywin</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">import jieba</span><br><span class="line"></span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class="line">from sklearn.cross_validation import cross_val_score</span><br><span class="line"></span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line"></span><br><span class="line">stopword_file = open(&apos;./stopword.txt&apos;)</span><br><span class="line">    </span><br><span class="line">stopword_content_string = stopword_file.read()</span><br><span class="line">    </span><br><span class="line">stopword_list = stopword_content_string.split(&apos;\n&apos;);</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">第一步，对于中文，需要进行分词处理</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 获取分类</span><br><span class="line">data_list = os.listdir(&apos;./data&apos;) #语料数据</span><br><span class="line">seg_path = &apos;./segment&apos; #分词后的数据</span><br><span class="line">seg_train_path = &apos;./segment/train&apos; #分词后的训练数据</span><br><span class="line">seg_test_path = &apos;./segment/test&apos;#分词后的测试数据</span><br><span class="line"></span><br><span class="line"># 获取各个分类内容，并进行切词保存在segment目录下</span><br><span class="line">def segment_content():</span><br><span class="line">    for data_cate in data_list:</span><br><span class="line">        content = &quot;&quot;</span><br><span class="line">        if not os.path.isdir(data_cate) and not data_cate == &apos;.DS_Store&apos;:</span><br><span class="line">            path = &apos;./data/%s&apos;%(data_cate)</span><br><span class="line">            files= os.listdir(path)</span><br><span class="line">            for file in files:</span><br><span class="line">                file_path = &apos;./data/%s/%s&apos;%(data_cate,file)</span><br><span class="line">                print(file_path)</span><br><span class="line">                if file == &quot;.DS_Store&quot;:</span><br><span class="line">                    os.remove(file_path)</span><br><span class="line">                else:</span><br><span class="line">                    f = open(file_path, mode=&apos;r&apos;, encoding=&apos;GB18030&apos;)</span><br><span class="line">                    try:</span><br><span class="line">                        content = f.read()</span><br><span class="line">                    except Exception:</span><br><span class="line">                        print(&apos;无法读取&apos;)</span><br><span class="line">                        continue</span><br><span class="line">                if not os.path.exists(&apos;%s/%s&apos;%(seg_train_path,data_cate)):</span><br><span class="line">                    os.mkdir(&apos;%s/%s&apos;%(seg_train_path,data_cate))</span><br><span class="line">                output = open(&apos;%s/%s/%s&apos;%(seg_train_path,data_cate,file), &apos;w&apos;)</span><br><span class="line">                content_seg = jieba.cut(content) # 使用jieba进行切词</span><br><span class="line">                output.write(&quot; &quot;.join(content_seg))</span><br><span class="line">                output.close()</span><br><span class="line"></span><br><span class="line">#从切分的数据中分离出测试数据集</span><br><span class="line">def cut_for_test():</span><br><span class="line">    seg_data_list = os.listdir(seg_train_path)</span><br><span class="line">    for data_dir in seg_data_list:</span><br><span class="line">         </span><br><span class="line">        if os.path.isdir(&quot;%s/%s&quot;%(seg_train_path,data_dir)):</span><br><span class="line">            file_dir_path = &quot;%s/%s&quot;%(seg_train_path,data_dir)</span><br><span class="line">            files = os.listdir(file_dir_path)</span><br><span class="line">            test_array = files[-10:]</span><br><span class="line">            for subfile in test_array:</span><br><span class="line">                if subfile == &quot;.DS_Store&quot;:</span><br><span class="line">                    os.remove(&quot;%s/%s&quot;%(file_dir_path,subfile))</span><br><span class="line">                    continue</span><br><span class="line">                if not os.path.exists(&quot;%s/%s&quot;%(seg_test_path,data_dir)):</span><br><span class="line">                    os.mkdir(&quot;%s/%s&quot;%(seg_test_path,data_dir))</span><br><span class="line">                shutil.move(&quot;%s/%s&quot;%(file_dir_path,subfile),&quot;%s/%s/%s&quot;%(seg_test_path,data_dir,subfile))          </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">第二步 获取训练集  返回训练集数据，类别列表</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">#获取训练数据集</span><br><span class="line">def loadtrainset():</span><br><span class="line">    allfiles = os.listdir(seg_train_path)</span><br><span class="line">    processed_textset  =[]</span><br><span class="line">    allclasstags = []</span><br><span class="line">    for thisdir in allfiles:</span><br><span class="line">        if not thisdir == &apos;.DS_Store&apos;:</span><br><span class="line">            </span><br><span class="line">            content_list = os.listdir(&apos;%s/%s&apos;%(seg_train_path,thisdir))</span><br><span class="line">            </span><br><span class="line">            for file in content_list:</span><br><span class="line">                if not os.path.isdir(&apos;%s/%s/%s&apos;%(seg_train_path,thisdir,file)) and not file == &apos;.DS_Store&apos;:</span><br><span class="line">                    path_name = &apos;%s/%s/%s&apos;%(seg_train_path,thisdir,file)</span><br><span class="line">                    print(path_name)</span><br><span class="line">                    f = open(path_name)</span><br><span class="line">                    processed_textset.append(f.read())</span><br><span class="line">                    allclasstags.append(thisdir)</span><br><span class="line">    return processed_textset,allclasstags</span><br><span class="line"></span><br><span class="line">#获得测试数据集</span><br><span class="line">def loadtestset():</span><br><span class="line">    allfiles = os.listdir(seg_test_path)</span><br><span class="line">    testset = []</span><br><span class="line">    testclass = []</span><br><span class="line">    </span><br><span class="line">    for cate_dir in allfiles:</span><br><span class="line">        if not cate_dir == &apos;.DS_Store&apos;:</span><br><span class="line">            test_content_list = os.listdir(&apos;%s/%s&apos;%(seg_test_path,cate_dir))</span><br><span class="line">            for test_file in test_content_list:</span><br><span class="line">                if os.path.isfile(&apos;%s/%s/%s&apos;%(seg_test_path,cate_dir,test_file)) and not test_file == &apos;.DS_Store&apos;:</span><br><span class="line">                    file_path_name = &apos;%s/%s/%s&apos;%(seg_test_path,cate_dir,test_file)</span><br><span class="line">                    print(&apos;获得测试数据文件名&apos;)</span><br><span class="line">                    print(file_path_name)</span><br><span class="line">                    f = open(file_path_name)</span><br><span class="line">                    testset.append(f.read())</span><br><span class="line">                    testclass.append(cate_dir)</span><br><span class="line">    return testset,testclass</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#第三步，处理词向量，包括去除停用词</span><br><span class="line">def content_vectorizer(data):</span><br><span class="line"></span><br><span class="line">    #载入停用词</span><br><span class="line">    # 语料向量化        </span><br><span class="line">    #if not os.path.exists(&quot;./vectorizer_content.m&quot;):</span><br><span class="line">    vectorizer_content__ = CountVectorizer(stop_words=stopword_list)</span><br><span class="line">    #vectorizer_content = joblib.load(&quot;./vectorizer_content.m&quot;)</span><br><span class="line">    x_array = vectorizer_content__.fit_transform(data)</span><br><span class="line">    #print(x_array.toarray())</span><br><span class="line">    # 计算各个分类内容的 tf-idf值</span><br><span class="line">    X_test =  TfidfTransformer().fit_transform(x_array) </span><br><span class="line">    print(&apos;content_vectorizer&apos;)</span><br><span class="line">    print(vectorizer_content__)</span><br><span class="line">    joblib.dump(vectorizer_content__, &quot;./vectorizer_content.m&quot;) </span><br><span class="line">    return X_test</span><br><span class="line"></span><br><span class="line">#第三步，处理词向量，包括去除停用词</span><br><span class="line">def content_vectorizer_test(data):</span><br><span class="line"></span><br><span class="line">    #载入停用词</span><br><span class="line">    # 语料向量化        </span><br><span class="line">    #if not os.path.exists(&quot;./vectorizer_content.m&quot;):</span><br><span class="line">    #需要使用和训练器相同的矢量器 否则会报错 ValueError dimension mismatch</span><br><span class="line">    vectorizer_content = joblib.load(&quot;./vectorizer_content.m&quot;)</span><br><span class="line">    #vectorizer_content = joblib.load(&quot;./vectorizer_content.m&quot;)</span><br><span class="line">    x_array = vectorizer_content.transform(data)</span><br><span class="line">    #print(x_array.toarray())</span><br><span class="line">    # 计算各个分类内容的 tf-idf值</span><br><span class="line">    X =  TfidfTransformer().fit_transform(x_array) </span><br><span class="line">    print(&apos;content_vectorizer_test&apos;)</span><br><span class="line">    print(vectorizer_content)</span><br><span class="line">    #joblib.dump(vectorizer_content__, &quot;./vectorizer_content.m&quot;) </span><br><span class="line">    return X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def file_tools(path):</span><br><span class="line">    content_data = []</span><br><span class="line">    f = open(path)</span><br><span class="line">    content = f.read()</span><br><span class="line">    content_seg = jieba.cut(content)</span><br><span class="line">    string_content = &quot; &quot;.join(content_seg)</span><br><span class="line">    content_data.append(string_content)</span><br><span class="line">    return content_data</span><br><span class="line"></span><br><span class="line"># 创建分类器</span><br><span class="line"></span><br><span class="line">def train():</span><br><span class="line">    from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">    </span><br><span class="line">    processed_textset,Y = loadtrainset()</span><br><span class="line">    #test_textset,Y_test = loadtestset()</span><br><span class="line">    X_train = content_vectorizer(processed_textset)</span><br><span class="line">    #X_test = content_vectorizer_test(test_textset)</span><br><span class="line">    # 多项式贝叶斯分类器</span><br><span class="line">    clf = MultinomialNB(alpha=0.001).fit(X_train,Y)</span><br><span class="line">    </span><br><span class="line">    # KNN分类器</span><br><span class="line">    #from sklearn.neighbors import KNeighborsClassifier    </span><br><span class="line">    #clf = KNeighborsClassifier().fit(X_train,Y)</span><br><span class="line">    # 随机森林</span><br><span class="line">    #from sklearn.ensemble import RandomForestClassifier    </span><br><span class="line">    #clf = RandomForestClassifier(n_estimators=8)    </span><br><span class="line">    #clf.fit(X_train,Y)  </span><br><span class="line">    </span><br><span class="line">    #predict_result = clf.predict(X_test) #预测结果</span><br><span class="line">    print(&apos;交叉验证结果&apos;)</span><br><span class="line">    print(cross_val_score(clf,X_train,Y,cv=10,scoring=&apos;accuracy&apos;))#交叉验证</span><br><span class="line">    </span><br><span class="line">    #训练完成，保存分类器</span><br><span class="line">    joblib.dump(clf, &quot;./分类器.model&quot;) </span><br><span class="line"></span><br><span class="line">#预测</span><br><span class="line">def predict(path):</span><br><span class="line">    check_file_data = file_tools(path)</span><br><span class="line">    check_file_vectorizer = content_vectorizer_test(check_file_data)</span><br><span class="line">    clf = joblib.load(&quot;./分类器.model&quot;)</span><br><span class="line">    predict_result = clf.predict(check_file_vectorizer)</span><br><span class="line">    print(&apos;------&gt;预测结果&lt;------&apos;)</span><br><span class="line">    print(predict_result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#segment_content()</span><br><span class="line">#cut_for_test()</span><br><span class="line">#train()#如果没有训练过模型，需要先通过此方法进行训练</span><br><span class="line">predict(&apos;./test2.txt&apos;)</span><br></pre></td></tr></table></figure><p></p><p>github:<a href="https://github.com/ilioner/ML-Classifier" target="_blank" rel="noopener">https://github.com/ilioner/ML-Classifier</a></p><p>参考:</p><ul><li><a href="http://blog.csdn.net/github_36326955/article/details/54891204" target="_blank" rel="noopener">http://blog.csdn.net/github_36326955/article/details/54891204</a></li><li><a href="https://www.cnblogs.com/taich-flute/p/6755031.html" target="_blank" rel="noopener">https://www.cnblogs.com/taich-flute/p/6755031.html</a></li></ul></div><div></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div><button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'><span>赏</span></button><div id="QR" style="display:none"><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/shouqian.jpg" alt="TywinZhang Alipay"><p>支付宝打赏</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/机器学习/" rel="tag">#机器学习</a> <a href="/tags/sklearn/" rel="tag">#sklearn</a> <a href="/tags/中文文本分类/" rel="tag">#中文文本分类</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/10/23/sklearn中CountVectorizer与TfidfTransformer使用案例.html" rel="next" title="sklearn中CountVectorizer与TfidfTransformer使用案例"><i class="fa fa-chevron-left"></i> sklearn中CountVectorizer与TfidfTransformer使用案例</a></div><div class="post-nav-prev post-nav-item"><a href="/2018/01/25/今天上午修整了一下SS.html" rel="prev" title="今天上午修整了一下SS">今天上午修整了一下SS <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="jiathis_style"><a class="jiathis_button_tsina"></a> <a class="jiathis_button_tqq"></a> <a class="jiathis_button_weixin"></a> <a class="jiathis_button_cqq"></a> <a class="jiathis_button_douban"></a> <a class="jiathis_button_renren"></a> <a class="jiathis_button_qzone"></a> <a class="jiathis_button_kaixin001"></a> <a class="jiathis_button_copy"></a> <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a> <a class="jiathis_counter_style"></a></div><script type="text/javascript">var jiathis_config={hideMore:!1}</script><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script></div><div id="uyan_frame"></div><script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2149408"></script></div></div><div class="comments" id="comments"><div class="ds-thread" data-thread-key="2017/11/15/ML：sklearn中文文本分类.html" data-title="ML：sklearn中文文本分类" data-url="http://yoursite.com/2017/11/15/ML：sklearn中文文本分类.html"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview">站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="TywinZhang"><p class="site-author-name" itemprop="name">TywinZhang</p><p class="site-description motion-element" itemprop="description">一个狮子座的不将就IT男青年</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">18</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><span class="site-state-item-count">2</span> <span class="site-state-item-name">分类</span></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">17</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ilioner" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub </a></span><span class="links-of-author-item"><a href="http://www.jianshu.com/users/899c84548f92/latest_articles" target="_blank" title="简书"><i class="fa fa-fw fa-globe"></i> 简书 </a></span><span class="links-of-author-item"><a href="https://profile-summary-for-github.com/user/ilioner" target="_blank" title="Git图谱1"><i class="fa fa-fw fa-globe"></i> Git图谱1 </a></span><span class="links-of-author-item"><a href="https://resume.github.io/?ilioner" target="_blank" title="Git图谱2"><i class="fa fa-fw fa-globe"></i> Git图谱2</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、基本概念"><span class="nav-number">1.</span> <span class="nav-text">一、基本概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、基本步骤"><span class="nav-number">2.</span> <span class="nav-text">二、基本步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-预处理"><span class="nav-number">2.0.1.</span> <span class="nav-text">1.预处理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-分词处理"><span class="nav-number">2.0.2.</span> <span class="nav-text">2.分词处理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-词向量处理"><span class="nav-number">2.0.3.</span> <span class="nav-text">3.词向量处理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-TF-IDF权重策略"><span class="nav-number">2.0.4.</span> <span class="nav-text">4.TF-IDF权重策略</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-建立分类器"><span class="nav-number">2.0.5.</span> <span class="nav-text">5.建立分类器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-评价分类结果"><span class="nav-number">2.0.6.</span> <span class="nav-text">6.评价分类结果</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">TywinZhang</span></div><div class="powered-by">由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动</div><div class="theme-info">主题 - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="http://cdn.bootcss.com/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="http://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js"></script><script type="text/javascript" src="http://cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script><script type="text/javascript" src="http://cdn.bootcss.com/velocity/1.2.1/velocity.min.js"></script><script type="text/javascript" src="http://cdn.bootcss.com/velocity/1.2.1/velocity.ui.min.js"></script><script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script><script type="text/javascript">var duoshuoQuery={short_name:"ilioner"};!function(){var t=document.createElement("script");t.type="text/javascript",t.async=!0,t.id="duoshuo-script",t.src=("https:"==document.location.protocol?"https:":"http:")+"//static.duoshuo.com/embed.js",t.charset="UTF-8",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(t)}()</script><script type="text/javascript">var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path;function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var searchFunc=function(e,c,s){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var t=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),a=document.getElementById(c),r=document.getElementById(s);a.addEventListener("input",function(){var u=0,d='<ul class="search-result-list">',f=this.value.trim().toLowerCase().split(/[\s\-]+/);r.innerHTML="",1<this.value.trim().length&&t.forEach(function(e){var a=!0,r=e.title.trim().toLowerCase(),c=e.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),t=e.url,s=-1,n=-1,o=-1;if(""!=r&&""!=c&&f.forEach(function(e,t){s=r.indexOf(e),n=c.indexOf(e),s<0&&n<0?a=!1:(n<0&&(n=0),0==t&&(o=n))}),a){u+=1,d+="<li><a href='"+t+"' class='search-result-title'>"+r+"</a>";var i=e.content.trim().replace(/<[^>]+>/g,"");if(0<=o){var l=o-20,p=o+80;l<0&&(l=0),0==l&&(p=50),p>i.length&&(p=i.length);var h=i.substring(l,p);f.forEach(function(e){var t=new RegExp(e,"gi");h=h.replace(t,'<b class="search-keyword">'+e+"</b>")}),d+='<p class="search-result">'+h+"...</p>"}d+="</li>"}}),d+="</ul>",0==u&&(d='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==f&&(d='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),r.innerHTML=d}),proceedsearch()}})};$(".popup-trigger").mousedown(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script></body></html>